{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards a Scientific Workflow Description: a yt Project Prototype for Interdisciplinary Analysis\n",
    "\n",
    "### NCSA & iSchool at University of Illinois, University College London\n",
    "- [yt-project.org](http://yt-project.org/)\n",
    "- [data-exp-lab.github.io](https://data-exp-lab.github.io/)\n",
    "- [github.com/yt-project](https://github.com/yt-project/yt/)\n",
    "- [github.com/data-exp-lab/analysis_schema](https://github.com/data-exp-lab/analysis_schema)\n",
    "\n",
    "#### Authors: Sam Walkow, Dr. Chris Havlin, Dr. Matthew Turk, Corentin Cadiou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Learning Cliff: Computational Workflows\n",
    "\n",
    "Working with scientific data and open source software requires understanding a myriad of tools and best practices. Analysis and visualization using python and open source relies on paradigms like imperative programming and for loops to write code and nuances in syntax like dot notation. After that, there's learning the internal functionality of individual packages, which are often domain specific. \n",
    "\n",
    "Interdisciplinary work provides an extra barrier, as new ways of thinking and new tools are added to the work load, butting heads with the domain focused tools available. While discovery and reconfiguration of software tools can be an intersection of creativity and innovation, too often learning curves get in the way and slow research down, or cause the wheel to be reinvented over and over again in each domain. \n",
    "\n",
    "**Scientific workflow description** provides an alternative to the cognitive overhead of learning a new software package and use of imperative programming paradigms often used with python. We aim to unite the core aspects of interdisciplinary computational work using common natural science mental models while removing inherent domain-specific workflow and computing language barriers, creating an accessible scientific query environment. **This description is encoded in a JSON schema, accessed by the user through a configuration file, and run using python modules that attach the configuration file to the code which produces output.** \n",
    "\n",
    "In this case, 'the code' is [yt](http://yt-project.org/), an open source python library designed for scientific analysis and visualization of volumetric data from the physical sciences. We use yt, an computational astrophyics tool, to demonstrate how a domain specific software can operate within a descriptive framework. \n",
    "\n",
    "Check out our previous work on understanding user mental models in yt here: https://samwalkow.github.io/2020-scipy-poster-domainstories/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing Scientific Analysis and Visualization\n",
    "\n",
    "What does it mean to *describe* a workflow? **We have developed a scientific workflow description prototype, which is divided into three different pieces - the JSON schema, a browser interface for user interaction, and a render-engine that will execute the code**. Using a configuration file which will live in the broswer, users can select actions and add appropriate data for those actions which are controled by the JSON schema. Users can then submit this file to the rendering engine, creating output for their work.  \n",
    "\n",
    "**Add visual**\n",
    "\n",
    "This creates a flexible, language-agnostic, and structured way for users to explore, analysis and visualize natural science data. It weaves human and language together, in a way that fits with how both human and machine *think* about the data and the program. Our work is currently supporting yt, but the schema could be extended to other python libraries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is yt?\n",
    "\n",
    "yt (https://yt-project.org/) is an open source python library designed for scientific analysis and visualization of volumetric data from the physical sciences. **The analysis schema is designed to operate on top of yt's existing functionalities, to provide intuitive and flexible access to yt operations.**\n",
    "\n",
    "Here's some yt examples:\n",
    "\n",
    "    Supernova Ignition Simulation\n",
    "    https://arxiv.org/abs/1807.07579\n",
    "    (Evan O’Connor and Sean Couch, MSU)\n",
    "    \n",
    "<img src=\"images/weatherradar.png\" align=\"right\" width=\"500\"/>\n",
    "\n",
    "<img src=\"images/supernova.png\" align=\"left\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                                 Predicted Weather Radar from Tornadogenesis Simulation\n",
    "                                                                                                   Leigh Orf (University of Wisconsin),\n",
    "                                                                                                                 Image Credit: NCSA AVL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using JSON as an Interface\n",
    "\n",
    "JSON as an interface also user's to access properties defined in the JSON schema, and add data that conforms to the schema's specifications, which in turn are submitted to the code behind the interface. **User's only need to understand JSON notation to use the code, or in this case yt. They don't need to know python or yt at all to use those tools in their analysis.** They can simply describe what they want the code to do in the JSON configuration file, and an output is returned. \n",
    "\n",
    "**The file is used as its own validation, as users objects that make up the workflow are validated against a JSON schema, either at runtime or (in certain code editors) in real time**. Workflows can be unique to the user but conform to a broad, domain specification, which makes work in the analysis schema ease to create and reproduce. \n",
    "\n",
    "Here's an example using VSCode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ytSlicePlotJSONEntry.gif\" align=\"center\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Analysis Schema\n",
    "\n",
    "The analysis schema design provides a structured grammar from which users can describe and tell the story of their data analysis. It's meant to connect with the users understanding of the data, while accessing the code it needs to run the analysis. **The schema has been developed to be flexible so users are free to complete operations in any order, while working in a structured schema which ensures the input to the JSON interface is valid and can be run in code**. This helps eliminate syntax issues, code run out of order issues, and lowers the barrier to usage. \n",
    "\n",
    "While the description are separate from yt, the mental model behind the data selection, transformation, and logic remain connected to yt. We change how users can access those operations, but not how yt handles the data it receives. In this way, users are learning how yt thinks and understands data input, while using description and their domain knowledge to reasoon with the data and the returned output.\n",
    "\n",
    "**By providing an additional layer between the user and the imperative code, he goal of the analysis schema is not to further separate the user from the software, but to bring them closer to together by improving the communication and shared work between human and machine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ytSlicePlotSchema.png\" align=\"center\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting Existing Software to Declarative Description\n",
    "\n",
    "How does the description reach and run the software? Data classes, type hinting, and schema models (all using Pydantic) are the core pieces that the analysis schema functions on. The schema is generated from yt data classes and type annotations, which is then accessed by users to guide and structure their analysis in the JSON interface. The JSON schema used for the description contains key words, which are relayed back to the code and used to run the code with user input as arguments. This code is run and output is displayed to the user. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ytSlicePlotJSONEntry.gif\" align=\"left\" width=\"500\"/>\n",
    "<img src=\"images/ytSlicePlotAnalysisSchema.gif\" align=\"center\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Contexts - Beyond Astrophysics\n",
    "\n",
    "Historically yt was developed for computational astrophysics, but we aim to expand yt's capabilities to support: \n",
    "\n",
    "- __Geophysics__\n",
    "- __Neuroimaging__\n",
    "- __Weather__ \n",
    "- __Climate__\n",
    "- __Oceanography__\n",
    "\n",
    "**Common elements and operations are often used across natural science domains** and we aim to identify and encode those operations into the analysis schema to make yt's visualization and analysis funcationality more accessable. \n",
    "\n",
    "Operations of analysis and visualization are often reused across domains including:\n",
    "- Loading and registering data into a coordinate system\n",
    "- Methods of aggregation\n",
    "- Methods of transformation\n",
    "- Methods of plotting\n",
    "\n",
    "Once we understand how users from other domains understand and process data, we can add logic to the schema that allow for the language and how yt handles the data to better match what users expect. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out some Geophsyics visualization and anlaysis with yt:\n",
    "\n",
    "https://nbviewer.jupyter.org/github/chrishavlin/AGU2020/blob/main/notebooks/cm1_supercell.ipynb\n",
    "\n",
    "https://github.com/chrishavlin/AGU2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sam Walkow, Informatics PhD Student at the University of Illinois\n",
    "\n",
    "Sam Walkow is a PhD student in Informatics interested in open source software sustainability, culture and data visualization. She's investigating these issues by looking at the intersection of human and computer workflows, problem solving and data conception and representation. \n",
    "\n",
    "#### Contact:\n",
    "- [@samwalkow](https://twitter.com/SamWalkow)\n",
    "- swalkow2@illinois.edu\n",
    "\n",
    "<img src=\"images/swalkow_dec2020.jpg\" align=\"left\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chris Havlin\n",
    "\n",
    "#### Contact:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matthew Turk, Assistant Professor in School of Information Sciences at the University of Illinois\n",
    "\n",
    "Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\n",
    "\n",
    "At the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.\n",
    "\n",
    "\n",
    "#### Contact:\n",
    "\n",
    "[Matthew Turk dot github dot io](https://matthewturk.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corentin Cadiou\n",
    "\n",
    "#### Contact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "We would like to thank major yt contributors including Nathan Goldbaum, and the larger yt community for their sustained effort.\n",
    "\n",
    "We would also like to thank the following entities for their support:\n",
    "\n",
    "- The Gordon and Betty Moore Foundation’s Data-Driven Discovery Initiative through Grant GBMF4561.\n",
    "- The National Science Foundation under Grants OAC-1663914 and ACI-1535651\n",
    "- NumFOCUS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "# <style>\n",
    "# @import url('https://fonts.googleapis.com/css?family=Nanum%20Gothic&display=swap');\n",
    "#    div.jp-MarkdownOutput {font-family: \"Nanum Gothic\"; font-size: 200%;}\n",
    "# </style>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnalysisSchema",
   "language": "python",
   "name": "analysisschema"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
